@document.meta
title: bnn101
description: For exploration of bnns prior to actual thesis implementation
authors: dabbing
categories: ceid
created: 2022-11-05
updated: 2022-12-14
version: 0.0.15
@end

* Checking out BNNs
** Figure out
   - [_] What tools are there for turning trained models to custom hardware?
   > So far seen _PYNQ_+(only relevant for xilinx© zynq platrforms)+ and _LeFlow_+(seems abandoned)+
   > I dont actually think those are gonna help, too general purpose and obtuse
   - [ ] Mathematical description of finding common patterns of wires in sums
   > Idea for doing it with graphs in notebook
   > Relevant thread: [Common subset algorithm]

** Hands-on
   Seems like the only real option is Larq
   - [x] Find a dataset suitable for printed electronics -using CNNs-
   > Proff. recomended the /UCI repository/(dataset on his papers).
   > So no CNNing any time soon…
   - [x] Extract weights from trained model
   > [Keras API]{https://keras.io/api/layers/}
   [Pickled weights]
   - [x] Test weights localy
   - [x] Use verilator
   > Need a *wrapper script* in cpp. Can pretty much copy-paste it.
   - [x] Prototype /parallel/ full precision * binary neuron
   - [x] Prototype /batch normalisation/ layer _as comparators_
   - [x] Test network up to xnor-pop 
   - [x] Merge into one module and format a bit
   - [x] Send trained network and unfinished verilog implementation to proff.
   - [x] Create ssh key and send it to proff. to request server acces
   > Mention realising last bnorm/softmax layer being a mistake due to requiring division
   > Mention plan to train with pure xnor-pop exit later
   > Mention most papers being more about memory usage etc
   - [x] Write email for progress report
   - [ ] Have version control for google colab ipynb scripts
   [Git in colab article]
   - [x] Train network with _ternary weights_
   - [x] Train network with less bits for activations
   - [ ] Iterative pruning with lottery-like retraining ternary network
   - [ ] Loss function to increase ammount of zeros
   _Generalise to heuristic for area/power cost_
   - [ ] Train with skip connections
   - [=] Train ensemble of bnns and prune them harder
   - [=] Minimise common partial sums [Common subset algorithm]
   I now think synopsys just doesn't need that much help
   - [ ] Find how to design analog circuits for popcount

*** Synopsys stuff
    - [x] Write a module with a random sum
    - [x] Measure area of single sum
    - [x] Have two completely unrelated sums for reference
    - [x] Test whether putting two sums with different order increases cost from just one
    - [x] Test whether putting an added term different between the two sums changes it
    *Result:* compiler is smart enough to change the order properly
    - [x] Test inlined pair with some operands same and some inversed versus with explicit
    sum of common and different elements
    - [x] Test inlined bunch of additions and subtractions *versus* explicit sum of positives minus
    sum of negatives
    - [ ] Test subtractions with comparison versus inversions with changed comparator
    - [x] Find deprecation warning to set correct "/focus on area/" flag
    Why does compile with high effort are give almost twice as much area as compile_ultra?
    Why do all costs increase when it's supposed to be optimising?

*** Batch normalisation prototype
    - [x] Read formula for comparator value from [XNOR Neural Engine] paper
    - [x] Implement formula in python script
    - [_] Save comparator values
    - [x] Test _comparison results_ match _Larq layer output_
    > Testing through python repl vs colabboratory sucks as a process
    - [x] Write script to print comparators from values
    > Propably will need to specify signed values
    - [x] Complete layer module
    - [x] Half-ass testbench
    - [x] Merge dense layer with /batch/binarization/

*** Dense fully binarised
    - [_] Write *popcount* module
    > Will just write a bunch of additions again
    > Letting the synthesiser do whatever it wants seems the plan
    > Straightforward additions take less power than wallace
    - [x] Test weights localy
    -- [x] Find how to pass activations straight to single layer in keras
    -- [x] Figure out what negative values from layer mean
    > To get (0,1)@x answer from (-1, 1)@y answer: x=(y+len(x))/2
    -- [x] Fix it calculates xor instead of xnor
    -- [x] Write /gold/ fuction to return keras layer's answer
    -- [x] Write function to get popcount answer from MAC answer
    - [x] Write module printer script
    -- [x] Edit fp*bin script
    - [x] Turn to full module
    - [x] Use bit ranges instead of array index
    - [x] Fix sign print error
    - [x] Test against Larq
    - [_] Add comparators
    - [_] Find formula for /x/ threshhold from /y/ one +see above+

*** A comparison of UCI datasets
    - Arrythmia: Low accuracy, biggest ammount of MACs
    - Cardio: Complicated data structure
    - -GasId-
    - HAR: Not sure how _time series data_ can train a NN on a _single sample_
    > Also just more data cleanup
    - *Pendigits*: Data in convinient int form and homogenous
    - Red/Whitewine: Terrible accuracy

*** Pendigits training
    - [x] Make model in Larq for /Pendigits/ dataset
    > Chosen because it seems simular to _MNIST_.
    > Architecture is 16->16->10
    > Binarised accuracy of ~0.7
    - [-] Synthesise the pendigits_draft design and get power/area data
    -- [x] Setup sshfs with server
    -- [=] Find how to change the timescale
    -- [x] Find where the printed library is located
    Its the _EGFET_ library in the default libpath
    -- [x] Set the correct library
    -- [x] Fix module names to match env file
    -- [x] Run synthesiser
    5.9k/6k net are in violation what's up
    -- [x] Copy tb
    -- [x] Run simulation
    Wait did it output anything
    -- [x] Run power
    Only leakage power reported, curious
    -- [_] Run area
    Its calculated on synthesys
    - [x] Train without batch-normalization
    Accuracy fell significantly, I will just keep it
    - [x] Train model with ternary weights
    - [x] Use keras tuner to decide on model hyperparameters
    - [x] Train with 4 bit accuracy (so /8)
    - [=] Play with threshold to balance zero-count with accuracy
    Currently threshold is handled by the tuner
    - [!] Change print-script to handle 0 values
    -- [ ] Check /_amaranth_/ library basics
    - [ ] Compare area and power of binarised and ternary


*** HAR training
    Accidentally used a different /HAR/ dataset than the paper used.
    36% accuracy, horrible. Won't move forward with that for the moment
    - [x] Download HAR dataset
    - [x] Shrink data
    Index and timestamps not useful, only use one device, ignore person
    So only keep sensor data.
    - [x] Check turning floating point to fixed point
    - [x] Turn datasets to fixed point
    - [_] Lookup ways to use collaboratory outside the browser
    SSH banned smh
    - [x] Find larq layers for ternary weights
    - [x] Save trained network

*** ONNX distraction
    - [x] Turn pendigits ternary model to ONNX
    - [x] Check contents of ONNX model
    - [ ] Get weights out of ONNX model
    - [x] Compile ONNX from tflite output of larq compute engine
    - [ ] Find what LCE_Quantization layer does exactly
** Reading up
   - [x] Read the [training bnns paper]{file:///home/dabbing/Desktop/Papers/bnn/Training_Binary_Neyral_Networks_With_Real_To_Binary_Convolutions.pdf}
   - [x] Read the bnn [meta-paper]{file:///home/dabbing/Desktop/Papers/bnn/A comprehensive review of Binary Neural Network.pdf}
   - [ ] Look for printed electronics paper implementations
   - [ ] Read the papers recomended on [larq guide]{https://docs.larq.dev/larq/guides/bnn-architecture/#further-references}
   - [_] Find papers for BNN accelarators
   +Tautology+
   - [!] Check referrers of early papers for cool papers
   - [x] Re-read printed electronics paper by prof.
   - [x] Read YodaNN paper
   > Mostly about caching inputs etc
   - [x] Read BNN comparison paper
   > *First* BNN accelarator paper
   > Maybe check referrers for later works
   - [x] Read [XnorBin]
   > Didn't I already read this
   - [x] Read [XNORPop]
   - [ ] Read [Bankman et. al]
   Useful for analog implementation
   - [x] Read [BinaryEnsembleNetwork]
   - [x] Read [Compressing Ternary Networks]
   *Very relevant to what I was trying to do with training*
   - [ ] Read [BNNs without BatchNorm]
   - [_] Read low latency with layer parallelisations
   > Not sure how relevant low latency is to me
   - [x] Read [FP-BNN paper]
   - [x] Learn about [Batch normalisation]{https://deepai.org/machine-learning-glossary-and-terms/batch-normalization}
   - [x] Learn about [Max pooling]{https://deepai.org/machine-learning-glossary-and-terms/max-pooling}
   - [x] Read up on [Ternary Weight Networks]{file:///home/dabbing/Desktop/Papers/bnn/Ternary_Weight_Networks.pdf}
   - [x] Learn about [skip connections]{https://www.analyticsvidhya.com/blog/2021/08/all-you-need-to-know-about-skip-connections/}
   - [x] Learn about [attention transfer]{https://arxiv.org/pdf/1612.03928.pdf}
   - [x] Learn about Popcount Compressor Trees
   - [x] Learn about Popcount reduction trees
   - [_] Search about softmax in bnns
   > Don't know if they are the same
   - [=] Read remaining papers from proff. recomendations
   > Gotta find 'em first tho...
   > THE FUCKING VPN DOESN'T DO SHIT
   - [x] Read [Fast Inference Framework]{file:///home/dabbing/Desktop/Papers/bnn/daBNN A Super Fast Inference Framework for Binary NeuralNetworks on ARM devices.pdf}
   > Not relevant
   - [x] Check ONNX format

** Components
*** Binary Convolution
**** XNOR
     Since it's with a constant weight, it's either
     nothing if /w/ is 1 or invertion if /w/ is 0.
     Is there a _verilog_ instruction to XNOR an array with a bit?
**** Popcount
     There should be quite a bit of common chunks of the same values being added.
     Locating these chunks among kernels in the same area(+could be translated+)
     would be useful.
*** Max Pooling
    Is there a way to avoid calculating all of them when I only need the max?
    > Can find max of 2 arrays by popcount/comparison of the bits they differ on one of them.
*** Batch Normalization
    Can turn it to a /comparator/ using a constant /c/.
    $c = -beta * \sqrt{var + eps} + mean$
    Since the value we compare is an integer, turning the comp value from float to int _does not introduce any error_

** References
   [AutoKeras]{https://autokeras.com/tutorial/image_classification/}
   [FP-BNN]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/FP-BNN-On-FPGA-neurocomputing.pdf}
   [Cardio Notebook]{https://www.kaggle.com/code/johnmantios/cardiotocography-classification}
   [Pickled weights]{file:///home/dabbing/Documents/BinaryNeuralNetwork/datasets/pendigits/pendigit_weights.pk}
   [XNOR Neural Engine]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/XNORNeuralEngine.pdf}
   [XNORPop]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/XNOR-POP A Processing-in-Memory Architecture for Binary Convolutional Neural Networks in Wide-IO2 DRAMs.pdf}
   [XnorBin]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/XNORBIN A 95 TOpsW Hardware Accelerator for Binary Convolutional Neural Networks.pdf}
   [Bankman et.al]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/An Always-On 3.8 μJ86% CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS.pdf}
   [Popcount accelarator]{file:///home/dabbing/Desktop/Papers/bnn/Accelarate/A 2.1GHz 6.5mW 64-bit Unified PopCountBitScan Datapath Unit for 65nm High-Performance Microprocessor Execution Cores.pdf}
   [Common subset algorithm]{https://softwareengineering.stackexchange.com/questions/259861/finding-common-subsets-in-a-list-of-sets}
   [Pendigits 2010]{file:///home/dabbing/Desktop/Papers/bnn/PendigitsPatternRecognitionLetters.pdf}
   [Git in colab article]{https://medium.com/geekculture/using-git-github-on-google-colaboratory-7ef3b76fe61b}
   [BinaryEnsembleNetwork]{file:///home/dabbing/Desktop/Papers/bnn/Binary Ensemble Neural Network More Bits per Network or More Networks per Bit.pdf}
   [Compressing Ternary Networks]{file:///home/dabbing/Desktop/Papers/bnn/Compressing  Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks.pdf}
   [BNNs without BatchNorm]
